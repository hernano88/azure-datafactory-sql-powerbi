# Proyecto 1. ETL en Azure: Data Factory + SQL DatabasePipeline de ValidaciÃ³n e Ingesta en Azure Data Factory  

Proyecto en **Azure Data Factory** que implementa un proceso de validaciÃ³n antes de la ingesta de datos, asegurando calidad y control en la carga de informaciÃ³n desde **Azure Data Lake**.  

---

## ğŸ¯ Objetivo  
Implementar un pipeline en Azure que:  
- Verifique la existencia de un archivo en Data Lake.  
- Obtenga metadatos del archivo (fecha de carga, tamaÃ±o, etc.).  
- Ejecute un flujo condicional:  
  - **True:** copia los datos y borra el archivo procesado.  
  - **False:** dispara un flujo de error controlado.  

---

## ğŸš€ Pipeline en Data Factory  
Pipeline orquestado que incluye:  
- Actividad de **ValidaciÃ³n de archivos**.  
- ObtenciÃ³n de metadatos con **Get Metadata**.  
- CondiciÃ³n **If** para ejecutar ramas de Ã©xito o error.  
- Actividad **Copy Data** para procesar el archivo cuando la validaciÃ³n es correcta.  

ğŸ“¸ Ejemplo del pipeline en Data Factory:  
![Pipeline de ValidaciÃ³n](pictures/validation_pipeline.PNG)  

---

## ğŸ›¢ï¸ Manejo de datos  
El pipeline gestiona los archivos en el **Azure Data Lake Storage** y asegura que solo los archivos vÃ¡lidos sean procesados, evitando duplicados o errores de carga.  

---

## ğŸ“Š Resultados esperados  
- EjecuciÃ³n controlada del flujo de ingesta.  
- Copia de archivos vÃ¡lida cuando se cumplen las condiciones.  
- Manejo automÃ¡tico de errores en caso de ausencia o problemas en los archivos.  

---

## ğŸ”§ TecnologÃ­as utilizadas  
- **Azure Data Factory**  
- **Azure Data Lake Storage**

# Proyecto 3. ETL en Azure: Data Factory + SQL Database

Proyecto de pipeline en **Azure Data Factory** para ingerir datos de COVID desde Data Lake y 
persistirlos en **Azure SQL Database**.  

---

## ğŸ¯ Objetivo
Implementar un pipeline en Azure que:
- Ingesta datos desde archivos CSV.
- Valida la presencia de los archivos en Data Lake.
- Copia y persiste los datos en una tabla de Azure SQL Database (`covid_population`).

---

## ğŸš€ Pipeline en Data Factory
Pipeline orquestado que incluye:
- ValidaciÃ³n de archivos.
- ObtenciÃ³n de metadatos.
- Actividad **Copy Data** para cargar informaciÃ³n en SQL.

ğŸ“¸ Ejemplo de actividad *Copy Data* en el pipeline:
![Copy Data Pipeline](pictures/copy_data_pipeline.PNG)

---

## ğŸ›¢ï¸ Persistencia en SQL Database
Los datos se almacenan en una tabla relacional dentro de **Azure SQL Database**, accesible vÃ­a editor de consultas.  

ğŸ“¸ Vista de la tabla con los datos cargados:
![SQL Database](pictures/sql_database.PNG)

---

## ğŸ“Š Resultados esperados
- Carga exitosa de datos en SQL Database.
- PreparaciÃ³n para su futura visualizaciÃ³n en Power BI.

---

## ğŸ”§ TecnologÃ­as utilizadas
- **Azure Data Factory**
- **Azure Data Lake Storage**
- **Azure SQL Database**



